| [Home](index.md) | [Lectures](lectures.md) | [Labs](labs.md) | [Assignments](assignments.md) | [Project](project.md)| [Contact](contact.md) |


## Lecture Slides

ISLR = Introduction to Statistical Learning
PDS = Python for Data Science
(see homepage for links)

1. Week 1
- September 3
  * [Intro to Data Science](lectures/Intro%20to%20data%20science.pdf)
  * [Data Transformation](lectures/data_transformations.pdf)
- September 5, 
  * [Tidy Data](lectures/tidy.pdf)
  * [Data Visualization](lectures/vis.pdf)
  * [Tidy Intro](lectures/pew.html), [Tidy Advanced](lectures/billboard.html), and
  [Vis](lectures/gapminder.html) notebooks

2. Week 2
- September 10
  * [Function Fitting Intro](https://observablehq.com/@krisrs1128/function-fitting)
  * [code examples](lectures/model_families_gallery.html)
  * Reading: ISLR 2.1, 3.2.1, 3.5
- September 12
  * [Function Fitting Algorithms](https://observablehq.com/@krisrs1128/function-fitting-crash-course)
  * [demo](https://observablehq.com/@krisrs1128/knn-bias-variance)
  * Reading: ISLR 3.2.1, 3.3.1, 3.3.2, 3.5, 4.3, 7.2, 8.1

3. Week 3
- September 17
  * Function Fitting Implementation
  * Reading: PDS pg. 390 - 396, 400 - 405 and ISLR 4.6.6
  * Optional reading: ISLR 8.3.1, 8.3.2
  * Notebooks: [Regression](https://colab.research.google.com/drive/1Ro8Jp975pBuW5DdljGmqXfMqSESFzfdY), [knn + logistic regression](https://colab.research.google.com/drive/1ZyUp1v7oaN8z0qk4Y-F_Dxz1TkBRFlNh), and [trees](https://colab.research.google.com/drive/1tv6npC_FnojKAo89zAHBWQColjYKQ-rd)
- September 19
  * [Unsupervised Learning Algorithms](https://observablehq.com/@krisrs1128/unsupervised-learning)
  * Reading: ISLR 10.1, 10.2, PDS pg. 462 - 476
  * Optional Reading: ISLR 10.3

4. Week 4
- September 24
  * [Cross Validation + Model Selection](https://observablehq.com/@krisrs1128/cross-validation-and-model-selection)
  * Reading: ISLR 5.1, PDS pg. 359 - 375
- September 26
  * [Introduction to Inference](https://observablehq.com/@krisrs1128/introduction-to-inference)
  * Reading: ISLR 3.1.1, 3.1.2, [Bayesian Basics](https://m-clark.github.io/bayesian-basics/) (intro - regression models)
  * Optional reading: [MSMB](http://web.stanford.edu/class/bios221/book/Chap-Testing.html) 6.1 - 6.6, [Statistics for Hackers](https://speakerdeck.com/jakevdp/statistics-for-hackers?slide=138)

5. Week 5
- October 1
  * [The Bootstrap](https://observablehq.com/@krisrs1128/the-bootstrap)
  * Reading: ISLR 5.2 - 5.3
  * Optional reading: [CASI](https://web.stanford.edu/~hastie/CASI_files/PDF/casi.pdf) Chapter 1
- October 3
  * [Large Scale Inference and Experimental Design](https://observablehq.com/@krisrs1128/large-scale-inference-and-experimental-design)
  * Reading: [MSMB](http://web.stanford.edu/class/bios221/book/Chap-Testing.html) 6.7 - 6.11, 13.1 - 13.4. 
  * Optional reading: [CASI](https://web.stanford.edu/~hastie/CASI_files/PDF/casi.pdf) 15.1 - 15.3

6. Week 6
- October 8
  * [Feature engineering + Outlier and error analysis](lectures/feature_engineering.pdf)
  * Reading: PDS 3
- October 10
  * [Feature selection](lectures/feature_selection.pdf) 
  * *the slides that are covered in the class are: slides 1 - 17 and 30 - 59*
  * Reading: [An Introduction to Variable and Feature Selection](http://www.jmlr.org/papers/volume3/guyon03a/guyon03a.pdf), [KDD tutorial](http://www.public.asu.edu/~jundongl/tutorial/KDD17/KDD17.pdf)
  * Optional reading: [Feature Selection for Data and Pattern Recognition](https://www.amazon.com/Feature-Selection-Recognition-Computational-Intelligence/dp/3662456192), [Computational Methods of Feature Selection](https://www.amazon.com/Computational-Methods-Selection-Knowledge-Discovery/dp/1584888784), [A Survey of Feature Selection Techniques](https://www.igi-global.com/chapter/survey-feature-selection-techniques/11077)
  * Feature selection tool [scikit-learn](https://scikit-learn.org/stable/modules/feature_selection.html), [featureminer](http://featureselection.asu.edu/featureminer.php)
  
7. Week 7 (Invited Guest Lectures)
- October 15
  * [Geospatial and Time series data analysis](lectures/geolocation.pdf) by [dr. Behrouz Babaki](https://behrouz-babaki.github.io/) 
  * Optional reading: [Python Geospatial Development](https://www.amazon.ca/Python-Geospatial-Development-Erik-Westra/dp/1849511543)
  * Tools: [Shapely](https://shapely.readthedocs.io/en/stable/manual.html), [Basemap](https://matplotlib.org/basemap/), [geopandas](http://geopandas.org/)
- October 17
  * [Privacy and Transparency in Machine Learning](lectures/privacy.pdf) by [dr. Ulrich Aivodji](https://aivodji.github.io/)
  * Optional reading: [Composition Attacks and Auxiliary Information in DataPrivacy](https://arxiv.org/abs/0803.0032), [A Survey Of Methods For Explaining Black Box Models](https://arxiv.org/abs/1802.01933), [Privacy risk in machine learning- Analyzing the connection to overfitting](https://arxiv.org/abs/1709.01604), [On the Protection of Private Information in Machine Learning Systems-Two Recent Approaches](https://arxiv.org/abs/1708.08022), [fairwashing-the risk of rationaliztion](https://arxiv.org/abs/1901.09749)
  * Protecting Privacy with MATH: [Video](https://www.youtube.com/watch?v=pT19VwBAqKA)
8. Week 8 
- October 22 
  * Reading week
  
9. Week 9
- October 31
  * **MIDTERM**: 16:30-18:30 at Z-110
  
10. Week 10
- November 5
  * Project presentation
  *
- November 7
  * [Algorithmic bias](lectures/algorithmic_bias.pdf): Source and types of data bias, bias and discrimination in machine Learning: fairness metrics 
  * Reading: [Social Data: Biases, Methodological Pitfalls, and Ethical Boundaries](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2886526), [Fairness definitions explained](http://fairware.cs.umass.edu/papers/Verma.pdf)
  * Video: [The Trouble with Bias](https://youtu.be/fMym_BKWQzk), [21 fairness definitions and their politics](https://youtu.be/jIXIuYdnyyk) 
  * Fairness metric tools: [The Aequitas Toolkit Paper](https://arxiv.org/abs/1811.05577), [The Aequitas Tool](https://github.com/dssg/aequitas)

11. Week 11
- November 12
  * [Text mining (NLP): part 1](lectures/NLP_part1.pdf): NLP pipeline, BOW, n-grams, TF-IDF, and language model
  * Reading: [Speech and Language Processing, Jurafsky and Martn, 2nd ed. (plus)](https://web.stanford.edu/~jurafsky/slp3/ed3book.pdf): Chapters 1 - 6
  * Tools: Python: [NLTK](https://www.nltk.org/), [Gensim](https://radimrehurek.com/gensim/), Java: Stanford CoreNLP, Apache OpenNLP
- November 14
  * [Text mining (NLP): part 2](lectures/NLP_part2.pdf): Word embeddings, SVD, Word2Vec and GLoVe.
  * Reading: [Word2Vec](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf), [GLoVe](https://nlp.stanford.edu/pubs/glove.pdf)
  * [Computer Vision: part 1](lectures/Computer_vision_part1.pdf): CV pipleline, Warping, Point processing, Filters
  * Reading: [Computer Vision:Algorithms and Applications, Richard Szeliski](http://szeliski.org/Book/drafts/SzeliskiBook_20100903_draft.pdf), Chapters 3.1.1, 3.2, 3.6.1
  * Tools: Python: [OpenCV](https://opencv.org/) 
  
12. Week 12
- November 19
  * [Computer Vision: part 2](lectures/Computer_vision_part2.pdf): Convolution 
  * Reading: [Computer Vision:Algorithms and Applications, Richard Szeliski](http://szeliski.org/Book/drafts/SzeliskiBook_20100903_draft.pdf), Chapters 3.1.1, 3.2, 3.6.1, [Filtering, Convolution, Correlation](https://courses.cs.washington.edu/courses/cse576/book/ch5.pdf), Chapter 5
- November 21
  * [Crash course to deep learning](lectures/deep_learning.pdf): Perceptrons, Neural networks, Convolution Neural networks, Recurrent Neural Network and LSTM
  * Reading: [Deep Learning, Ian Goodfellow, Yoshua Bengio, Aaron Courville](http://deeplearning.cs.cmu.edu/document/reading/book1.zip), Chapter 6, 9, 10
  * [ConvNet notes](http://cs231n.github.io/convolutional-networks/)
  * Tools: [Tensorflow](https://www.tensorflow.org/), [Keras](https://keras.io/), [Pytorch](https://pytorch.org/)
  
13. Week 13
- November 26
  * [Graph ML: part 1](lectures/graph_learning_part1.pdf): node embeddings: adjajency matrix,matrix factorization, multi-hop embedding, random walk embeddings, and node2vec
  * Reading: [Representation learning on graphs](https://arxiv.org/abs/1709.05584)
  * Tools: Python: [igraph](https://igraph.org/redirect.html), [NetworkX](https://networkx.github.io/)
  * Tool: embedding tools: [node2vec](https://snap.stanford.edu/node2vec/), [deepwalk](https://github.com/phanein/deepwalk), [GraphVite](https://graphvite.io/)
- November 28
  * [Graph ML: part 2](lectures/graph_learning_part2.pdf): Graph neural networks, and Graph convolutional networks
  * [Multimodal Learning](lectures/multimodal_learning.pdf): Multimodal machine learning, Multimodal Representation, Fusion
  * Tools: [Deep autoencoders in Keras](https://medium.com/datadriveninvestor/deep-autoencoder-using-keras-b77cd3e8be95), [Ensemble methods: scikit-learn](https://scikit-learn.org/stable/modules/ensemble.html#ensemble)
  * Reading: [Multimodal Machine Learning: A Survey and Taxonomy](https://arxiv.org/abs/1705.09406)
  
14. Week 14
- December 3
  * Final project presentation

15. Week 15
- December 10
  * **FINAL**: 16:30-18:59 at Z310 and Z330


